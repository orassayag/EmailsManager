Goal:
=====
The goal of this project is to scan the gmail MBOX files (that contains all the inbox/sent email messages).
The process will contain verification of the email messages count, and all email addresses count, validate
each email address, and in the end, export all email addresses to a TXT file. Doing all of this,
without any traditional database involved. Also, to build a script that merge couple of merged TXT files,
and pull out their diff.

Overall Hints:
==============
-When developing: each day to a backup of this project.
-Once finish the crawl task, always keep on local PC the last one "all_email_addresses" with date of the running.
-Read the instructions in the "README.md" file.
-Each new script - Write instructions in the "README.md" file.

Search & Crawl Email Addresses Script Tasks:
============================================
-Build the steps structure of the entire processes: *
-setup step: *
------------ *
-Validate all parameters from the settings file. *
-Create the mongo database. *
-initiate step: *
-------------- *
-Off test mode: Do a logic to check if no internet connection. If so, cancel all the processes. *
-Check if can write on PC (like the crawl-mbox-files-initiate.service.js step). *
-Validate the mongo db connection is working (add row, check that exists, and delete it). - TODO
-On test mode: Load all the TXT files to randomize. *
----LOOP on processes count---- *
-preProcess step: *
----------------- *
-Generate the search key. *
-Preform a logic to check that the generated search key was not already exists in the processes list search keys. *
-Add maximum number of times to generate key. After that - throw exception. *
-If it's first search process - Set global start time. *
-Set the page index. *
-Create the SearchProcess model with all the relevant properties. *
-Get the last Id of email row from the database in order to continue to insert to the database from the last Id.
 If table not exists or empty table - Start with 0. - TODO
-Log the process count out of index with colors and the search key + start time. *
-Return the SearchProcess model. *
-Set the source utils: if it's test mode - Load all pages to source.utils.js. *
-searchEngine step: *
------------- *

this.allLinksList *

-Write comments on all the settings in the settings file. *
-Add links fields like the email addresses fields. *
-Set timeout time in milliseconds for the request. - TODO
-Set maximum fetched links count limit and check it. *
-Set maximum list links count limit and check it. *
-Verify that the result cherecters not execceeded the maximum limit. *
-Build method to concat string array (check if exists any element and concat, else return the original array). *
-On test mode: Just load random data from all the search engine TXT file exists. *
-Add logic - If 0 page links were found, stop the process and continue to the next process. *
-Off test mode: Build logic with Puppeteer.js to scan search engines by search
 engine type and the number of pages that configured (also take from old spider the parameters to build the search key). - TODO
-Off test mode: Build enum of the search engine key and the URL address to search with placeholders. - TODO
-Log: the number of total links count fetched. *
-filter step: *
------------- *

this.filteredLinksList *
this.invalidLinksList *
this.finalLinksList *

-Filter links from domain list. *
-Add logic to check if the URL is a valid one - Cancelled. *
-Insert the link according to: filtered, invalid, final, skipped, duplicate. *
-Log: the number of filtered / invalid / relevant links count fetched. *
----LOOP on link pages fetched---- *
-searchPage step: *
----------------- *

this.allEmailAddressesList *

-On the searchPage process decide if to skip or not, on this page, according to the characters length.
-Add delay between each crawl of pages.
-Set timeout time in milliseconds for the request.
-Set maximum fetched email addresses count limit and check it.
-Set maximum list email addresses count limit and check it.
-Verify that the result characters not exceeded the maximum limit.
-Use the built method to concat string arrays.
-On test mode: Just load random data from all the page source TXT file exists.
-Off test mode: Build logic with Puppeteer.js to scan page for email addresses.
-Log: Process number / index processes, search engine type, search key, page number / page indexes, the total email addresses count fetched.
-In both cases, return a list of email addresses.
-Try catch on all.
-modify step:
-------------

this.filteredEmailAddressesList *
this.invalidEmailAddressesList *
this.fixedDomainEmailAddressesList *
this.fixedOtherMistakesEmailAddressesList *
this.duplicateEmailAddressesList *
this.finalEmailAddressesList *

-Loop on each email address and:
-Add logic to check if the email address is a valid one.
-Add extra logic to filter email addresses (check on old spider?).
-Add logic to filter specific domain.
-Edit the email address: keep the key name, and the domain to be lower.
-Add logic to fix domain mistakes (check on old spider? + search on Google).
-Add logic to fix other email addresses mistakes.
-Return the email addresses list.
-Add logic - If 0 email addresses were found, stop the process and continue to the next process.
-Add logic to remove duplicate email addresses and get the count of duplicates that have been removed.
-Log: currentPageIndex / totalPagesIndex - total | filtered | invalid | fix domain | fix miskate | duplicates | final
-Try catch on all.
-sync step:
-----------

this.newEmailAddressesList *
this.existsEmailAddressesList *

-Loop on each email address and:
-Check if the email address exists on the database.
-If yes, don't insert it.
-If no, insert it.
-Try catch on all.
-Add to configuration the number of delay between each communication with the data base. Default is 1 second.
-Log: Process number / index processes, search engine type, search key, page number / page indexes, the number of exists / new email addresses.
-Check if summary step or next page round / process round.
-Move to next page / process.
-summary step:
--------------
-In the end of each process:
-Add short summary mode option: only relevant email addresses in the summary log and no summary processes log.
-Log a summary to a TXT file about the process data. The format name of the file SummaryProcess_{process_number}_{today_date}_{milliseconds}.txt
-Include all parameters fields.
-The finalEmailAddressesList will be the only list that will be single line. The rest will be new line + comma.
-postProcess step:
-----------------
-Add the process to the processes list in the logic file.
-Log the process count out of index with colors and the generated search string + end time.
-If it's last process - set global end time.
-summary processes step:
------------------------
-On the last process, run this step.
-Calculate the details and sums of all search processes.
-Log to a TXT file a global statistics for all processes (all details and statistics data).
-Include all the names of the summary processes TXT files.
-Include the total statistics of all fields in the models.
-The format name of the file SummaryProcesses_{process_number}_{today_date}_{milliseconds}.txt

Puppeteer.js:
-Build the rest of the logic without Puppeteer.js package.
-Take the old spider that has been developed in .NET C# and see if anything worth taking or if
 something is missing. All relevant sources place in comment here.
-Logic tasks to add when Puppeteer.js can work:
-Test logic to fetch HTML source page.
-Test logic to get all email addresses from HTML source page.
-Test MongoDb CRUD operations.
-Add logic to support SHORT_MODE (not need to grab all summary statistic data each step, only the minimum data).
-Start to build the logic as the following:
-Test on Puppeteer.js with bing.com with URL parameters / with click and different inputs.
-Add logic to scan HTML source page and pull out all the email addresses from it into an array.
-Add logic to scan all HTML href links and fetch the full URL address into an array.
-Next, find an NPM package that crawl HTML source page by a given URL.
-The source should be given after full load of the source page, not by "view source".
-Each round, take the last row in the database and continue the Id from the last Id number.
-Find a search engine that don't propmpt camptcha and can be controlled
 by dynamic parameter search in the URL address and by manipulating it.
-Next, build the logic to crwal email addresses by low level search engines by
 dynamic parameter search in the URL address and by manipulating it.
-The final step is to initiate a mongodb database, with simple table of "emailAddresses".
-The fields: Id, EmailAddress (the Id field will be generated by programmed index by the code itself)
-Add a check if the table exists. If not, create it (initiate step).
-Add a boolean flag to drop the table and re-create it (initiate step).
-Add another sub-script task, to export all the email addresses in the table into a TXT file (merge view).
-Do all the ToDo points.
-Refactor log title step method. *
-Limits:
--------
-global:
-Set timeout time in milliseconds for the request.
-Set maximum charecters for source length.
-Search engine
-Set maximum links count limit and check it.
-searchPage:
-Set maximum email addresses count limit and check it.
-filter Step
-Add logic - If 0 page links were found, skip to the next process.
-modify:
-Add logic - If 0 email addresses were found within 3 pages (configured number) - Stop the process and continue to the next process.
-sync:
-Add to configuration the number of delay between each communication with the data base. Default is 1 second.
-------
-Check all files for bugs and format all files.
-Format all multiply parameters to be in one line
-IMPORTANT!!!!! In the end of the development, IT'S IMPORTANT!!!!! to write instuctions how to run the script,
 and what each settings does, and what is each parameter in the summary file mean.

Add Support For Crawling Multi MBOX Files Logics:
=================================================
-In the beginning - Log the file name that it working on. *
-Add support for multi MBOX files. *
-"pre-process" step: Log the MBOX file process, start the first date time (if it's the first time),
 Check if there is more than 1 MBOX file to process. If not, the "post-process" will not do anything.
-"post-process" step: Calculate the global summary, set the end time in the last summary file.
-Build anoter step called "global summary" that will log all the summary of all MBOX files processes.
-In the initiate step, create the GlobalSummaryData object and set the number of files and sizes.
-For each MBOX file that scanned, create separate directory in the dist directory.
-Multi MBOX files - Each MBOX file separate table results, and in the end - Total results.
-Create a new class to support the summary of all the processes.
-Calculate the summary of all files in the end.
-Add directory creation for each MBOX file.
-Add validation of maximum number of MBOX files to process.
-In case of multi MBOX files, add option to merge between all of them to 1 final TXT file in the end.
-Add a logic that if only one MBOX file processed, no need for summary log in the end of all MBOX files.
-Add a summary for all files.
-Re-Write the README.md file.
-Re-Write the package.json file.
-Update the goal in this document.
-Copy the goal on this document to the README.md file.
-All require - Sort by alphabetic order.
-In the end of development - Empty the source directory.
-Add comments in the all code in the end (main places + methods).
-Check spelling and the grammar of these instructions.

Compare TXT Files Script:
=========================
-Add support for compare between 2 TXT files that contains email addresses lists.
 (Add identical count and difference count).
-Create one TXT file of difference email addresses.
-Pull out summary file.
-When finish to build this script, add option to run it automaticlly after the "crawl mbox files" script.

Extra API External Verification Script:
=======================================
-Build other script to test email addresses by external API.
-Validate that the URL works.
-Add address of file to verify the email addresses.
-In addition to the regular verification, all the valid email addresses needs to pass additional verification.
-Use this tool: https://github.com/whois-api-llc/node-email-verifier
-Register with list of email addresses and build key-value list of API keys.
-If reached the limit, try the next one.
-Write error on the specific account that failed.
-For example, try with theses:
or.assayag@osrenterprises.com
at_CZ09Ne9NqzYUsXGxfE8GTIdCYWMWu

Email Address Validation Script:
================================
-Add regex utils.
-Build a method that verify that an email address is valid, and fix it if needed:
-First thing in this method, after the @ sign, lowercase all the rest of the email.
-Build a script to that get an email address and validates it.
-The first step is to validate basic stuff, like empty, domain, @, and name.
-The second step is to validate the email address within as much regular expressions as possible (1000 regular expressions is good).
-The final step is to validate the email address with 2 external APIs to validate the email.
-Make an option to check only with the regular expressions, without the API.
-Log a summary about the email address with the percentage.
-For this script build the regex utils.
-Once finish with this task, need to add this validation (without any API) involvment in the validation step.