-Remove all comments. *
-Remove all unused files, functions, and variables. *
-Remove setup service - not relevant anymore. *
-Run again to verify all works as expected. *
-Remove all comments again. *
-Make sure spaces in all files between bottom, need tso fix cases like this: *
} *
module.exports *
} *
export default *
-Re-create package.json-lock, update package.json with full structure like from crawler.json. *
-Add to README.MD links to GitHub, LinkedIn, StackOverFlow - Take from 'crawler' project. *
-Make the misc directory - External packages, error index, todo_tasks, complete_tasks.txt. *
-Move all complete tasks from here to complete_tasks.txt. *
-Rewrite error numbers. *
-Remove all debugger. *
-Make sure all comments ends with . *
-Format all files. *
-Verify all works. *
-Reorder here the tasks. *
-Fix TXT files spell mistakes. *
-Fix all spell mistakes in all TXT files. *
-Fix spell mistakes in all txt files. *
-Re-upload to GitHub + StackOverFlow (Developer Story). *
-Update on iOmega. *
-Change the project name to 'mbox-crawler'. *
-Change all directories to lower case. *
-Change all the project file names to lower case. *
-Update all the packages. *
-Implement the backups logic from 'crawler' project. *
-Move this document into 'misc' directory inside the project itself. *
-Implement log-update logic, take from 'crawler' project - Canceled. *
-Upload to GitHub. - Canceled. *
-Deploy on Heroku. - Canceled. *
-Also deploy email-searcher on Heroku. After verify it works - Remove from previous domain (from danitronen1920@gmail.com + update it on KeyPass) + Add to bookmark - Canceled. *
-Rename all classes. *
-Implement updated .eslint file from 'crawler' project. *
-Remove all empty spaces between lines. *
-Change all const x = new Class + retrun x to just return new Class();. *
-Remove all things not related to mbox crawler logic + remove unused NPM packages + all unused functions. *
-Remove all the logic of search email addresses, keep only the logic of reading mbox files. *
-Remove any thing related to OSR. *

============================

Goal:
=====
The goal of this project is to scan the gmail MBOX files (that contains all the inbox/sent email messages).
The process will contain verification of the email messages count, and all email addresses count, validate
each email address, and in the end, export all email addresses to a TXT file. Doing all of this,
without any traditional database involved. Also, to build a script that merge couple of merged TXT files,
and pull out their diff.

Overall Hints:
==============
-When developing: each day to a backup of this project.
-Once finish the crawl task, always keep on local PC the last one "all_email_addresses" with date of the running.
-Read the instructions in the "README.md" file.
-Each new script - Write instructions in the "README.md" file.

Search & Crawl Email Addresses Script Tasks:
============================================
-Build the steps structure of the entire processes: *
-setup step: *
------------ *
-Validate all parameters from the settings file. *
-Create the mongo database. *
-initiate step: *
-------------- *
-Off test mode: Do a logic to check if no internet connection. If so, cancel all the processes. *
-Check if can write on PC (like the crawl-mbox-files-initiate.service.js step). *
-Validate the mongo db connection is working (add row, check that exists, and delete it). - TODO
-On test mode: Load all the TXT files to randomize. *
----LOOP on processes count---- *
-preProcess step: *
----------------- *
-Generate the search key. *
-Preform a logic to check that the generated search key was not already exists in the processes list search keys. *
-Add maximum number of times to generate key. After that - throw exception. *
-If it's first search process - Set global start time. *
-Set the page index. *
-Create the SearchProcess model with all the relevant properties. *
-Get the last Id of email row from the database in order to continue to insert to the database from the last Id.
 If table not exists or empty table - Start with 0. - TODO
-Log the process count out of index with colors and the search key + start time. *
-Return the SearchProcess model. *
-Set the source utils: if it's test mode - Load all pages to source.utils.js. *
-searchEngine step: *
------------- *

this.allLinksList *

-Write comments on all the settings in the settings file. *
-Add links fields like the email addresses fields. *
-Set timeout time in milliseconds for the request. - TODO
-Set maximum fetched links count limit and check it. *
-Set maximum list links count limit and check it. *
-Verify that the result cherecters not exceeded the maximum limit. *
-Build method to concat string array (check if exists any element and concat, else return the original array). *
-On test mode: Just load random data from all the search engine TXT file exists. *
-Add logic - If 0 page links were found, stop the process and continue to the next process. *
-Off test mode: Build logic with Puppeteer.js to scan search engines by search
 engine type and the number of pages that configured (also take from old spider the parameters to build the search key). - TODO
-Off test mode: Build enum of the search engine key and the URL address to search with placeholders. - TODO
-Log: the number of total links count fetched. *
-filter step: *
------------- *

this.filteredLinksList *
this.invalidLinksList *
this.finalLinksList *

-Filter links from domain list. *
-Add logic to check if the URL is a valid one - Cancelled. *
-Insert the link according to: filtered, invalid, final, skipped, duplicate. *
-Log: the number of filtered / invalid / relevant links count fetched. *
----LOOP on link pages fetched---- *
-searchPage step: *
----------------- *

this.allEmailAddressesList *

-On the searchPage process decide if to skip or not, on this page, according to the characters length.
-Add delay between each crawl of pages.
-Set timeout time in milliseconds for the request.
-Set maximum fetched email addresses count limit and check it.
-Set maximum list email addresses count limit and check it.
-Verify that the result characters not exceeded the maximum limit.
-Use the built method to concat string arrays.
-On test mode: Just load random data from all the page source TXT file exists.
-Off test mode: Build logic with Puppeteer.js to scan page for email addresses.
-Log: Process number / index processes, search engine type, search key, page number / page indexes, the total email addresses count fetched.
-In both cases, return a list of email addresses.
-Try catch on all.
-modify step:
-------------

this.filteredEmailAddressesList *
this.invalidEmailAddressesList *
this.fixedDomainEmailAddressesList *
this.fixedOtherMistakesEmailAddressesList *
this.duplicateEmailAddressesList *
this.finalEmailAddressesList *

-Loop on each email address and:
-Add logic to check if the email address is a valid one.
-Add extra logic to filter email addresses (check on old spider?).
-Add logic to filter specific domain.
-Edit the email address: keep the key name, and the domain to be lower.
-Add logic to fix domain mistakes (check on old spider? + search on Google).
-Add logic to fix other email addresses mistakes.
-Return the email addresses list.
-Add logic - If 0 email addresses were found, stop the process and continue to the next process.
-Add logic to remove duplicate email addresses and get the count of duplicates that have been removed.
-Log: currentPageIndex / totalPagesIndex - total | filtered | invalid | fix domain | fix mistake | duplicates | final
-Try catch on all.
-sync step:
-----------

this.newEmailAddressesList *
this.existsEmailAddressesList *

-Loop on each email address and:
-Check if the email address exists on the database.
-If yes, don't insert it.
-If no, insert it.
-Try catch on all.
-Add to configuration the number of delay between each communication with the data base. Default is 1 second.
-Log: Process number / index processes, search engine type, search key, page number / page indexes, the number of exists / new email addresses.
-Check if summary step or next page round / process round.
-Move to next page / process.
-summary step:
--------------
-In the end of each process:
-Add short summary mode option: only relevant email addresses in the summary log and no summary processes log.
-Log a summary to a TXT file about the process data. The format name of the file SummaryProcess_{process_number}_{today_date}_{milliseconds}.txt
-Include all parameters fields.
-The finalEmailAddressesList will be the only list that will be single line. The rest will be new line + comma.
-postProcess step:
-----------------
-Add the process to the processes list in the logic file.
-Log the process count out of index with colors and the generated search string + end time.
-If it's last process - set global end time.
-summary processes step:
------------------------
-On the last process, run this step.
-Calculate the details and sums of all search processes.
-Log to a TXT file a global statistics for all processes (all details and statistics data).
-Include all the names of the summary processes TXT files.
-Include the total statistics of all fields in the models.
-The format name of the file SummaryProcesses_{process_number}_{today_date}_{milliseconds}.txt

Puppeteer.js:
-Build the rest of the logic without Puppeteer.js package.
-Take the old spider that has been developed in .NET C# and see if anything worth taking or if
 something is missing. All relevant sources place in comment here.
-Logic tasks to add when Puppeteer.js can work:
-Test logic to fetch HTML source page.
-Test logic to get all email addresses from HTML source page.
-Test MongoDb CRUD operations.
-Add logic to support SHORT_MODE (not need to grab all summary statistic data each step, only the minimum data).
-Start to build the logic as the following:
-Test on Puppeteer.js with bing.com with URL parameters / with click and different inputs.
-Add logic to scan HTML source page and pull out all the email addresses from it into an array.
-Add logic to scan all HTML href links and fetch the full URL address into an array.
-Next, find an NPM package that crawl HTML source page by a given URL.
-The source should be given after full load of the source page, not by "view source".
-Each round, take the last row in the database and continue the Id from the last Id number.
-Find a search engine that don't prompt captcha and can be controlled
 by dynamic parameter search in the URL address and by manipulating it.
-Next, build the logic to crawl email addresses by low level search engines by
 dynamic parameter search in the URL address and by manipulating it.
-The final step is to initiate a mongodb database, with simple table of "emailAddresses".
-The fields: Id, EmailAddress (the Id field will be generated by programmed index by the code itself)
-Add a check if the table exists. If not, create it (initiate step).
-Add a boolean flag to drop the table and re-create it (initiate step).
-Add another sub-script task, to export all the email addresses in the table into a TXT file (merge view).
-Do all the ToDo points.
-Refactor log title step method. *
-Limits:
--------
-global:
-Set timeout time in milliseconds for the request.
-Set maximum charecters for source length.
-Search engine
-Set maximum links count limit and check it.
-searchPage:
-Set maximum email addresses count limit and check it.
-filter Step
-Add logic - If 0 page links were found, skip to the next process.
-modify:
-Add logic - If 0 email addresses were found within 3 pages (configured number) - Stop the process and continue to the next process.
-sync:
-Add to configuration the number of delay between each communication with the data base. Default is 1 second.
-------
-Check all files for bugs and format all files.
-Format all multiply parameters to be in one line
-IMPORTANT!!!!! In the end of the development, IT'S IMPORTANT!!!!! to write instructions how to run the script,
 and what each settings does, and what is each parameter in the summary file mean.

Add Support For Crawling Multi MBOX Files Logics:
=================================================
-In the beginning - Log the file name that it working on. *
-Add support for multi MBOX files. *
-"pre-process" step: Log the MBOX file process, start the first date time (if it's the first time),
 Check if there is more than 1 MBOX file to process. If not, the "post-process" will not do anything.
-"post-process" step: Calculate the global summary, set the end time in the last summary file.
-Build another step called "global summary" that will log all the summary of all MBOX files processes.
-In the initiate step, create the GlobalSummaryData object and set the number of files and sizes.
-For each MBOX file that scanned, create separate directory in the dist directory.
-Multi MBOX files - Each MBOX file separate table results, and in the end - Total results.
-Create a new class to support the summary of all the processes.
-Calculate the summary of all files in the end.
-Add directory creation for each MBOX file.
-Add validation of maximum number of MBOX files to process.
-In case of multi MBOX files, add option to merge between all of them to 1 final TXT file in the end.
-Add a logic that if only one MBOX file processed, no need for summary log in the end of all MBOX files.
-Add a summary for all files.
-Re-Write the README.md file.
-Re-Write the package.json file.
-Update the goal in this document.
-Copy the goal on this document to the README.md file.
-All require - Sort by alphabetic order.
-In the end of development - Empty the source directory.
-Add comments in the all code in the end (main places + methods).
-Check spelling and the grammar of these instructions.

Compare TXT Files Script:
=========================
-Add support for compare between 2 TXT files that contains email addresses lists.
 (Add identical count and difference count).
-Create one TXT file of difference email addresses.
-Pull out summary file.
-When finish to build this script, add option to run it automatically after the "crawl mbox files" script.

Extra API External Verification Script:
=======================================
-Build other script to test email addresses by external API.
-Validate that the URL works.
-Add address of file to verify the email addresses.
-In addition to the regular verification, all the valid email addresses needs to pass additional verification.
-Use this tool: https://github.com/whois-api-llc/node-email-verifier
-Register with list of email addresses and build key-value list of API keys.
-If reached the limit, try the next one.
-Write error on the specific account that failed.

Email Address Validation Script:
================================
-Add regex utils.
-Build a method that verify that an email address is valid, and fix it if needed:
-First thing in this method, after the @ sign, lowercase all the rest of the email.
-Build a script to that get an email address and validates it.
-The first step is to validate basic stuff, like empty, domain, @, and name.
-The second step is to validate the email address within as much regular expressions as possible (1000 regular expressions is good).
-The final step is to validate the email address with 2 external APIs to validate the email.
-Make an option to check only with the regular expressions, without the API.
-Log a summary about the email address with the percentage.
-For this script build the regex utils.
-Once finish with this task, need to add this validation (without any API) involvement in the validation step.
-Try to run with small mbox file and list tasks need to be complete. *
-Change all places to use index.js + replace in all places. *
-Fix all compilations issues. *
-Add comments to settings parameters. *
-Update packages npm outdated --depth 9999. *
-Re-check all imports in the entire project (check for duplicate imports paths). *
-Sort alphabetically all imports: external packages, settings, (containers, components, store, assert) data, models, enums, services, utils. *
-Sort alphabetically by https://alphabetizer.flap.tv/. *
-Add 'Misc.' section to gitignore and npmignore files (include tasks txt files + backups directory). *

Search & Crawl Email Addresses Script Tasks:
============================================
-Add new script file called "search-email-addresses.js". *
-Add the script to the package.json file. *
-Logic without Puppeteer.js (local text HTML page sources): *
-Get 10 different types of bing.com sources. *
-Get 10 different types of links HTML page sources. *

Add Support For Crawling Multi MBOX Files Logics: *
================================================= *
-Open a backup project and initiate it. *
-Change the "totalEstimateEmailMessagesCount" to be a determine number, not estimated. *
-Add the index of total files count in the title of the MBOX file title. *
-Set the start time in the first file process. *
-Add to the statistic data a method to prepare display data. *
-Add another 2 steps called "pre-process", and "post-process". *
-Add the file name in each step. *

Complete Extra Crawl MBOX Files Logics: *
======================================= *
-Fix bug with duplicates function. *
-Test with amount of email addresses less then the minimum. *
 (less than 99, less than 100, less than 101 + in the settings the limit is 99, and also 100, and also 101, for example). *
-Handle case and test it - Where only one file created, or less then maximum email addresses collected. *
-Add logic that if email messages counts equals - Display only one of them. *
-Test with empty MBOX file (add validation of minimum size, and number of minimum email addresses - at least 1). *
-Find new name to "lastTXTFilesCount". *
-Remove all code-comments from all over the place, put them on tests.js file. *
-Test errors during streaming. All error cases in each streaming file or append file, *
 or any fs action (where the process can continue). *
-Same sentence for all errors. *
-Change all "email" to "emailAddress". *
-Change all "emails" to "emailAddresses". *
-Change all "emailsItems" to "emailMessages". *
-Change all "array" key to "list". *
-All places of "Emails" change to "Email Messages". *
-All "Emails" change to "Email Addresses". *
-Change the name of the script from "mbox-files-crawler" to "crawl-mbox-files" in all places. *
-Decide if text / mbox files or MBOX / TXT files. *
-In the summary description, add '.' in the end. *
-Go through all initialize number, change from "0" to null. *
-All places of "Emails" change to "EmailAddresses" - Cancelled. *
-All "throw new Error" convert to errorUtils.js (Build new function for message only) - Cancelled. *
-Remove the "error.utils" and throw exception. *
-Find solution foe the throwError function and regular "new Error". *
-Change "email" to "email address" or "email message". *
-In all places, first it's the email messages, than the email addresses. In this order. *
-All "merged" and all "ed" change to present time. *
-Do all the "ToDo" points around the code. *
-Change "isNotEmptyArray" to "isExists". *
-Change the "text-files-difference" script name. *
-Change all url to upper case. *
-Write plan to crawl email addresses script. *
-Build a function that check multi parameters with multi error codes, and implement it on all the validations steps - Canccelled. *
-Same function but to numbers. Implement it in settings, for example, and also on all the validation steps - Canccelled. *

Complete Crawl MBOX Files Script: *
================================= *
-Convert the MergeData to MergeData and MergeRound, like ScanData and ScanRound. *
-Validation in merge step. *
-Add statistics parameters in each step of the progress (verifications and summary). *
-In the final table of all summary, each parameter of number will be with comma. *
-Each task will be activated by different script. *
-Validation in validation step. *
-Validation in finalize step. *
-Change all "validate" actions to "validating". *
-Add error number to "Unmatch scan results". *
-In the scan step, check that the scan step have 2 rounds. *
-In the merge step, check that at least 1 round exists. *
-In the finalize step, check that all data elements exists. *
-Write description on each step in the logic file. *
-Add minimum file size validation. *
-Fix setup validation future errors. *
-In the scan step, check that at least 1 email address scanned. *
-In each step, validate all results. *
-Validation in scan step. *
-Validation in crawl step. *
-In the crawl step, check that at least 1 email address crawled. *
-In the end of development - Remove the backup_sources directory. *
-Refactor MboxFileData and TextFileData to 1 class. *
-Refactor all times fields to a class. *
-In the summary step - Log all the results into a log file (key: value (comment) + new line). *
-In the end, write a summary (with table) of all the statistics. *
-Add comment that explain what it the meaning of the parameter. *
-Include comment about the field in the object. *
-Change the order of the summary writing file and the summary log, and add the summary file in the log. *
-In the scan step, compare the "initiateScanLinesCount" to the scan lines count. *
-Build a function to log summary into a text file in a more pretty way: *
-Take the longest key length, add it 5, and complete with spaces for other keys. *
-Rename the project to "EmailsManager". *
-Go through all backups and rename the project. *
-Move the "backup_sources" outside the project. *
-Remove all "backup_sources" directory from all backups directories. *
-Fix alphabetic order of the merged and lists of the emails. *
-Convert all times to displayable times. *
-Optimize all functions / method to receive 1 object parameter. *
-Add missing validations in all utils functions (ToDo comments). *
-Change the "createFileName" to be in the order of the parameter's use. *
-On the logProgress - Change all parameters to have the comma function (check if its a number, of course). *
-Create a function that check if array exists and have elements in it. *
-Validate that if no mbox file, throw error. *
-In merge.js there is a place to merge parts of the code to a method. *
-Replace all numbers of tasks in '-'. *
-Write instructions to use the script in the README file. *
-Bring back colors utils. *
-Move the enum function to separate file. *
-Add main file to load multiply path (services, utils, models). *
-On logics, refactor titles for each step with function "step". *
-Change the return results to be the file. *
-In each step use "process.stdout.write("hello: ");" to update the progress during the process. *
-Global logic that repeats themselves - Convert to global functions. *
-After the scan process, add another step called "confirm", to confirm all limits of the mbox files. *
-Create the script name in system.enum.js file and use it in the settings. *
-In the initiate step, create the directory with the same name on the dist directory. *
-In the initiate step, validate that the directory with the same name on the source directory exists. *
-Set up maximum count of emails, maximum lines count, and mbox file size validation. *
-The statistics file name will be the same as the original file. *
-In the initiate step, add validation of enough free space according to the number of mbox files to scan. *
-In the beginning, add the list of files about to scan, filter only the mbox files. *
-One step after the validation step, add the finalize step, which will include: *
1. Verify that the final merge, the valid, the invalid files exists. *
2. Remove all the temporary text files from the crawl step. *
5. Add "final_all_emails" text file, with all email addresses, separated by break line (no date needed). *
6. Add "merged_all_emails", a copy of the final_merged_file. *
7. Add the current date to all the main files. *
8. On the validation / all steps step move all settings to the settings file. *
-Before the table log - Build a method that calculate a few statistics, like: *
1. On merge step - Count the number of email addresses, calculate the total number of email addresses that have been removed. *
2. In the end, subtract the start time from the end time, and calculate the total time of the entire process. *
-In the end of each file process, delete the temporary files. *
 Keep only the merged, statistics, valid, and invalid files. *
-Delete all temporary files in the end of the process. *
-In the beginning - Log the number of mbox files found, total width of all of them together. *
-Color each major status change step. *
-When finish to scan the entire mbox file, continue to the next step. *
-The third step is to build and perform the following logic: *
1. Loop on all text files. *
2. Each fetch, inject all the email addresses array into set. *
3. When the set reaches 5,000 emails, append it to a text file. *
4. In the end, if less then 5,000, append the rest to a text file. *
5. Repeat the process several times, each time duplicate the *
   number of email addresses to reach, until it will be a one text file. *
6. Each round list the round number, number of files, limit reached. *
7. The name of the final file will be different. *
-Functions that repeats themselves in several files - Move to global service. *
-Logic that repeats itself in same file - Move to a local function. *
-Display the list of all mbox files in the initiate step in *
 table package, and the total files count that about to be crawled. *
-In the final line of the table list the total number of files and the size of all files together. *
-Limit the characters mbox file name in all places. *
-In the setup step, validate all configurations, and add validation that the URL on the validation *
 step is available and online (test it with sample email address for true and false). *
-Store all the configurations in external files (by categories) (include the validation URL also there). *
-Validate that the numbers in the settings are positive. *
-Add validation to all configuration numbers, take it from VisualizationTool project. *
-Change "statistics" to "summary" (file name and all). *
-The next part is to scan each and every email address of the email addresses by request to check. *
1. Check if the final file exists. *
2. In a new email addresses array, all valid email addresses will be stored + count the valid email addresses. *
3. All invalid email addresses will be counted for the summary in the end. *
4. All invalid email addresses will be append to other file. *
5. Sort the email addresses before writing to text file. *
6. Get the email addresses count in each file, and size and the name of each file. *
7. Limit the number of characters in the email address to display in the console line in the validation step. *
-Add to the initiate step the check on the access of paths: *
 https://nodejs.org/api/fs.html#fs_fs_access_path_mode_callback *
-Add for size and name fields a display field. *
-Add the initiate line number count to the scan data. *
-Convert the scan data to be an object that contain scanRounds array. *
-Refactor the validation step. *
-Add percentage display to the 2 scan, the crawl, and the validate steps according to the following logic: *
1. In scan step: Add lines counter for the mbox file. Then, in the first scan (line by line) calculate percentage with that. *
   In the second step, according to the first scan email addresses count, calculate the percentage with that. *
2. In the crawl step: calculate percentage according to the email addresses count. *
3. In the validate step: calculate percentage according to the email addresses count. *
-Move the "mbox-files-crawler-handler.service.js" to the email.utils file. *
-Declare all the relevant variables: path of the file, number of lines to scan, ect. *
-The first step is to count all lines, and count all email addresses, *
 start and end time of process, get the file size and file name *
-Compare it with the mbox package with the same process. *
-The second step is to build and perform the following logic: *
1. Scan a number of email addresses, and store them in string. *
2. When reach to a number of email addresses, pause the stream. *
3. Count the email addresses pulled out + the number of lines scanned so far. *
4. Compare with original number. If invalid, throw exception. *
5. Remove all duplicates email addresses from the array. *
6. Append all email addresses stored in the array into a text file, separated by comma. *
7. Count the number of text files with a counter. *
8. Empty the lines string to avoid memory leak. *
9. Resume the stream and continue to store the next number of lines. *
10. Compare the number of lines scanned and the number of lines pulled out. *
11. If the number of lines scanned or the number of email addresses pulled out not equals *
    to the numbers from the first step, throw exception and stop the process. *
-Change all "start / finish" comment step to "start end" with the step name. *
-Re-arrange all paths from initiate step to avoid duplicate logic. *
-Convert all to models and validators. *
-For each mbox file, append all the valid email addresses from the last array into a final *
 text file, named by the original mbox file name, into a special directory, *
 separate each email address by break line. *
-Add start time and end time, and total time that took, for each mbox
 file (in addition to the total time of all the process files). *

Not Relevant Tasks / Cancelled Tasks:
=====================================
-Handle case of exceptions.
-Remove the summary object from the file object.
-In each step, check that the element data exists.
-In each step, compare the number of email addresses to the first scan number of email addresses. If unmatch - Throw error.
-Convert the summary keys to a new method - 1. Upper first letter. 2. Split capital letters.
-In the finalize step, create a function to validate all results.
-Go through all places and pull out objects from duplicate "this.file".
-Change the statistic parameter that has more then one number to be an object.
-Re-number all the tasks order.
-Add timestamp of the status line.
-Change the statistic parameters to be objects.
1. Convert all text files objects into an array.
2. Remove the "final_merged" file from the merge step in the finalize step.
-Within each error case, add exception or error that could happen and test it.
-Throw exceptions in random places in the steps, and see that happens. Fix if needed.
1. Scan all the text files, get the number of email addresses and the number of lines.
2. Compare with the number from the first step.
3. Read all the text files.
4. Store all the scanned lines in an array.
5. Store the number of lines scanned and the number of email addresses scanned for the summary in the end.
6. Compare the number of emails, number of lines of the file, with the first scan. If not equal, throw exception.
-The forth step is to convert all the scanned lines array into an array of email addresses.
-From the new email addresses array, remove all the duplicates email addresses.
-Store the number of email addresses (after the duplicates has been removed) for the summary in the end.
1. Scan each text file.
2. Get out all the email addresses.
3. Move them to set, to remove duplicates.
4. Inject them into final array.
5. In the end, repeat the process once again (convert to set) in the final array.
-Remove the "test.js" file after everything is working.
-Add support for diff between several text files that contains email addresses list.
 (Add identical count and diff count).
-Base text file, and new text file to find diffs.
-Also, build logic to append new text file to old file.
-Also, build logic to append new text file to old file, and delete the old file.
-In the final merged step, make a copy with the data today for backup purposes reasons.
-Add support for diff between 2 text files that contains email addresses list.
 (Add identical count and diff count).
-Build a logic to merge between 2 text files.
-If not working (memory leak):
1. Convert the original mbox file to source file.
2. Check if it's mbox file or text file mode according to the script.
3. Merge the old merged file and the new merge file, and build the same logic as the mbox file.
4. Upgrade the mbox steps to support text files according to the script.
-Rename the final merge file to "all_emails" with the date of today.
 This file need to be kept with the MBOX files.
-Merge Text Files Task:
-Add support for merge between 2 text files that contains email addresses lists.
 (Add duplicate count and diff count).
-Remove duplicates.
-Pull out summary file.